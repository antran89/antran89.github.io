<html>
<!-- Stolen from Georgia Gkioxari -->
<head>
  <style type="text/css">
    /* Stolen from Sergey stolen from Jon Barron */
    body
    {
      background-color:#FFFFFF;
      color:#222222;
    }
    a {
      color: #1772d0;
      text-decoration:none;
    }
    a:focus, a:hover {
      color: #f09228;
      text-decoration:none;
    }
    body,td,th {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 13px
    }
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700
    }

    h5 {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700;
      color: orange;
    }
  </style>

  <title>Action Tubes</title>

</head>

<body>
  <h2>Actionness-assisted Recognition of Actions</h2>
  <h4>Oct, 2015</h4>

  <p style="text-align: center;">
    <img height=266 src="fat.png" />
  </p>

  <h4>Abstract</h4>
  <p>
    We elicit from a fundamental definition of action low-level attributes that can reveal agency and intentionality. These descriptors are mainly trajectory-based, measuring sudden changes, temporal synchrony, and repetitiveness. The actionness map can be used to localize actions in a way that is generic across action and agent types. Furthermore, it also groups interacting regions into a useful unit of analysis, which is crucial for recognition of actions involving interactions. We then implement an actionness-driven pooling scheme to improve action recognition performance. Experimental results on various datasets show the advantages of our method on action detection and action recognition
    comparing with other state-of-the-art methods.
    <br /><br />
    [ <a href="../../papers/YeLuo_ICCV_2015.pdf"; target="_blank"> <strong>Paper</strong> </a> ], [ <a href="../../papers/YeLuo_ICCV_2015.bib"; target="_blank"> <strong>BibTex</strong> </a>]
  </p>

  <hr />
  <h4>Method</h4>
  <p>
    Describe briefly the actionnes method here.
  </p>


  <hr />
  <h4> Results</h4>

  <p>
    Images of action detection and action recognition will be here.
  </p>  

  <hr />
  <h4>Code</h4>
  <p>
    The code will be coming soon.
  </p>


  <hr />
  <h4>How to cite</h4>

  <p>
    If you find our work useful, please cite it. The bibtex entry is provided below
    for your convenience.
    <br /> <br />
    [ <a href="../../papers/YeLuo_ICCV_2015.pdf"; target="_blank"> <strong>Paper</strong> </a> ], [ <a href="../../papers/YeLuo_ICCV_2015.bib"; target="_blank"> <strong>BibTex</strong> </a>]
  </p>

  <p>

  </p>


  <hr />
  <h4>Contact</h4>
  <p>
    For any questions regarding the work or the implementation, contact the authors at 
    <ul>
      <li><a href="https://sites.google.com/site/luoyentu/"; target="_blank">Ye Luo</a></li>
      <li><a href="http://howtobeahacker.github.io/"; target="_blank">An Tran</a></li>
      <li><a href="https://www.ece.nus.edu.sg/stfpage/eleclf/"; target="_blank">Loong-Fah Cheong</a></li>
    </ul>
  </p>

</body>

</html>
